{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f186c1e8-59fa-4eba-8173-05e8416ab907",
   "metadata": {},
   "source": [
    "## TensorFlow 2 quickstart for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfa78f2-049d-450a-945c-a1b26b7cd420",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow)\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow)\n",
      "  Using cached keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow)\n",
      "  Using cached optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m567.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:26\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m702.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m696.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m660.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m669.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m658.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m670.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m588.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m655.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m595.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m596.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m488.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.1/349.1 kB\u001b[0m \u001b[31m589.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m486.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m664.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.65.1 keras-3.4.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.0 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 rich-13.7.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 werkzeug-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25d74e9-ef10-49ec-bc08-786334c6c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e9bc72-9e84-4376-ba19-65715d8c4807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8efd7-0382-44db-a763-b9314f0d2300",
   "metadata": {},
   "source": [
    "加載並準備 MNIST 數據集。圖像的像素值範圍是從 0 到 255。通過將這些值除以 255.0 將它們縮放到 0 到 1 的範圍，將樣本數據從整數轉換為浮點數：  \n",
    "\n",
    "因為浮點數有更高的精度，更適合用於神經網絡的計算。  \n",
    "\n",
    "數據集被存儲在 Google 的雲存儲中，TensorFlow 團隊為了方便用戶提供的數據集下載鏈接。  \n",
    "mnist.npz 是一個 NumPy 格式的壓縮文件，內含訓練集和測試集的圖像數據及其對應的標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf44a9b-e54d-4f13-bb3c-5f38fa9ec752",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "# 加載 MNIST 數據集，會自動檢查本地是否已經存在\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 將像素值縮放到 0 到 1 的範圍\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f95392-8b61-41e9-89e5-6f9514ca3093",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78876dca-a914-49e0-a1c2-b777e78cacf2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be13c6-5964-4f9f-aacd-83b95f3cc782",
   "metadata": {},
   "source": [
    "* Keras 提供了一個簡單且一致的 API，使得構建和訓練神經網絡變得更加容易。相比於直接使用低層 TensorFlow 操作，Keras 的高層 API 可以讓開發者更加專注於模型設計而不是底層實現細節。\n",
    "* Keras 的模型是模塊化的，可以輕鬆地組合不同的層、激活函數、優化器等，這使得實驗和調試變得更快捷。\n",
    "\n",
    "* layers 和 models 是 Keras 中的核心模塊，使用這行代碼可以簡化在構建神經網絡模型時的代碼書寫，使代碼更加簡潔和易讀。\n",
    "\n",
    "* Keras 的 layers 模塊包含了構建神經網絡層的所有基本組件，比如 Dense 層、卷積層、池化層等。models 模塊則包含了構建和編譯模型的工具，比如 Sequential 和 Model 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19e1de2-eb2e-4bdb-8c5b-78be74c1e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307fb163-60a6-4a63-96ad-1ae61b12be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220468c4-8acf-4f4c-8c38-20ae183f4624",
   "metadata": {},
   "source": [
    "註：若模型中的最後一層要用softmax，則改成：layers.Dense(10, activation='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e1bdfe0-20c9-4536-9e7f-23caf078e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23420343,  0.11045586, -0.07002977,  0.25856367, -0.2247243 ,\n",
       "        -1.020943  ,  0.6223849 ,  0.34172758,  0.35149777,  0.675537  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d5f9b-05d5-4399-b581-d9c4edfc13a0",
   "metadata": {},
   "source": [
    "tf.nn.softmax 函數將 logits 轉換為每個類別的概率。  \n",
    "softmax 函數的作用是將一個向量轉換為一個概率分佈，使得每個值都在 0 到 1 之間，且所有值的總和為 1。這在分類問題中非常有用，因為它可以將模型的輸出轉換為可解釋的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c76269f5-7ca8-4064-badd-4e7d4c342673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06620129, 0.09344372, 0.07801287, 0.10836089, 0.0668318 ,\n",
       "        0.03014324, 0.15591149, 0.11775793, 0.11891409, 0.16442269]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd752d-45a6-403a-95f8-66a5dae32637",
   "metadata": {},
   "source": [
    "Define a loss function for training using losses.SparseCategoricalCrossentropy:  \n",
    "\n",
    "模型輸出是 logits，而不是已經通過 softmax 處理的概率。\n",
    "\n",
    "SparseCategoricalCrossentropy適用於分類問題。  \n",
    "設置 from_logits=True 時，損失函數會將輸入視為未經過 softmax 函數處理的 logits，此時損失函數的計算公式為$loss=-log(\\frac{e^{z_i}}{\\sum_{j}e^{z_j}})$。這種設置確保了計算的數值穩定性，避免了在計算 softmax 和損失時可能出現的數值不穩定問題。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a3e2ab-74f7-497d-8e01-793b93feb966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.losses.losses.SparseCategoricalCrossentropy at 0x7fa5d19c3610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0910c1e-bac0-4c88-bbec-1199cfefa8d9",
   "metadata": {},
   "source": [
    "用來計算損失函數在給定輸入上的輸出結果。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c76a1d0-cd80-4be5-9ce2-ffee45dad6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5017948"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss_fn 為損失函數，在上方被定義。\n",
    "# y_train 是訓練數據的標籤。\n",
    "# y_train[:1] 取的是訓練數據標籤中的第一個樣本。\n",
    "# predictions 是模型對一些輸入數據的預測結果，通常是 logits。\n",
    "\n",
    "# 計算 y_train[:1] 和 predictions 之間的損失。\n",
    "# y_train[:1] 是真實標籤，predictions 是模型的預測輸出（logits）。\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0ed91-00a6-4ebb-b6d9-3590342629e0",
   "metadata": {},
   "source": [
    "進行模型訓練之前，使用 Keras 的 Model.compile 方法配置和編譯模型是至關重要的。這個過程確保了模型在訓練過程中能夠使用指定的優化器、損失函數以及評估指標。  \n",
    "* 設定優化器 (Optimizer)  \n",
    "  優化器是用來更新模型權重的算法。adam 是一種常用的自適應學習率優化算法，具有較好的性能和收斂速度。這個優化器會根據梯度信息自動調整學習率。\n",
    "* 設定損失函數 (Loss Function)  \n",
    "  損失函數用於計算模型預測結果與實際標籤之間的誤差。在之前的步驟中，已經定義了一個損失函數 loss_fn。這個損失函數會根據模型的輸出和實際標籤計算損失值，用於指導模型的訓練過程。\n",
    "* 設定評估指標 (Metrics)  \n",
    "  評估指標用於在訓練過程中監控模型的性能。accuracy 是一個常見的指標，用於測量模型的預測正確率。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db199bed-dcb1-4e3e-bfd0-5d89d05503f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b2a08-eed7-4e0b-bac4-3d58e0873c00",
   "metadata": {},
   "source": [
    "## Train and evaluate your model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86019e2-423b-4a6e-abbf-115d75180b07",
   "metadata": {},
   "source": [
    "model.fit 方法用於訓練模型，它會迭代地更新模型的權重以最小化損失函數。這個方法接受多個參數，其中最基本的包括訓練數據、標籤和訓練的輪數（epochs）。\n",
    "* x_train  \n",
    "  這是模型用來學習的輸入數據。\n",
    "* y_train  \n",
    "  這是每個樣本的真實標籤。\n",
    "* epochs  \n",
    "  訓練的輪數。每輪訓練包括了使用所有訓練數據一次。這裡的 epochs=5 表示模型將會在整個訓練數據上訓練 5 輪。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfa846-0c52-4fa4-a7bd-c926f2ebdb48",
   "metadata": {},
   "source": [
    "### model.fit 的運行過程  \n",
    "1. 初始化訓練：\n",
    "   * 開始訓練過程，模型的權重會根據初始設置進行初始化。\n",
    "2. 迭代訓練：\n",
    "   * 在每一個 epoch 中，模型會通過所有的訓練數據。這過程中，模型會使用優化器計算梯度，並更新權重以最小化損失函數。\n",
    "   * 每個 epoch 都會計算並記錄訓練過程中的損失和評估指標（如準確率）。\n",
    "3. 更新權重：\n",
    "   * 在每個批次（batch）中，模型的權重會被更新。這是基於該批次數據的損失值和梯度計算來完成的。\n",
    "4. 訓練結束：\n",
    "   * 當所有的 epochs 完成後，訓練過程結束。模型的最終權重會基於所有 epochs 的累積更新結果來確定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b3987d5-b785-46fa-8431-c1b979a954f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.4943\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1509\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9662 - loss: 0.1073\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9723 - loss: 0.0877\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.0726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa5d19e3510>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32d314-9360-4716-b9b6-767fc206cc5f",
   "metadata": {},
   "source": [
    "Model.evaluate 方法用於評估模型的性能，這個方法通常用於驗證集或測試集上，以檢查模型在這些新數據上的表現，有助於理解模型在訓練之外的數據上的準確性和效果。  \n",
    "1. x_test\n",
    "   * 這是用來評估模型性能的輸入數據。在這個例子中，x_test 是 MNIST 數據集中的測試數據（圖像），形狀為 (num_test_samples, 28, 28)。\n",
    "2. y_test\n",
    "   * 這是對應於 x_test 的真實標籤。在這個例子中，y_test 是每張測試圖像所對應的數字標籤，值為 0 到 9 之間的整數。\n",
    "3. verbose\n",
    "   * 控制顯示進度的詳細程度。\n",
    "     * verbose=0：不顯示進度條。\n",
    "     * verbose=1：顯示進度條。\n",
    "     * verbose=2：顯示每個 epoch 的進度信息（更詳細）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2955139d-6945-4320-9b19-4aa6ec4e5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9742 - loss: 0.0870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07492344081401825, 0.9776999950408936]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "123a5555-6018-46d8-ab10-b71b10c19b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - 1ms/step - accuracy: 0.9777 - loss: 0.0749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07492344081401825, 0.9776999950408936]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb7f5d-40f4-41a3-a184-e4b67ec20ae0",
   "metadata": {},
   "source": [
    "probability_model 定義了如何將 Softmax 層附加到已經訓練過的模型上，從而將模型的輸出轉換為概率。  \n",
    "\n",
    "1. tf.keras.Sequential:\n",
    "   * 這是一種線性堆疊模型的方式。它允許你按順序添加層，每一層的輸出作為下一層的輸入。\n",
    "2. model:\n",
    "   * 這是你之前定義並訓練好的模型。它的最後一層是沒有 Softmax 的，因此它輸出的是 logits，即每個類別的原始分數。\n",
    "3. tf.keras.layers.Softmax():\n",
    "   * 這是一個 Softmax 層，將模型的 logits 轉換為概率。Softmax 函數會將每個輸出值轉換為一個範圍在 [0, 1] 之間的值，這些值的總和為 1。這樣每個輸出值就可以解釋為該類別的預測概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88b72c9c-d7f9-4745-9a1e-52b23805f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義一個新的模型來附加 softmax\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model, # 添加訓練好的模型\n",
    "  tf.keras.layers.Softmax() # 在最後添加 softmax 層\n",
    "])\n",
    "# probability_model 將返回概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6b3b902-4a92-463e-b7be-e3b5da50b0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[2.84399590e-07, 3.03455172e-09, 8.98396138e-06, 3.65933054e-04,\n",
       "        6.33458347e-12, 4.60282251e-07, 1.94427638e-14, 9.99617577e-01,\n",
       "        6.49230092e-09, 6.73752493e-06],\n",
       "       [3.33506250e-07, 1.49640284e-04, 9.99723017e-01, 1.26358616e-04,\n",
       "        2.54291132e-14, 1.64139152e-07, 1.88044353e-07, 5.85454536e-13,\n",
       "        2.22729057e-07, 4.21029449e-14],\n",
       "       [3.87061121e-08, 9.99527097e-01, 4.11563269e-05, 3.44077620e-07,\n",
       "        4.94278211e-05, 6.52550216e-06, 2.21942537e-05, 3.43160704e-04,\n",
       "        9.33864521e-06, 7.29520536e-07],\n",
       "       [9.99992728e-01, 1.73897789e-11, 1.68775614e-07, 6.37276321e-09,\n",
       "        1.80174684e-08, 1.36880061e-07, 6.68090206e-06, 1.16750705e-07,\n",
       "        1.41076373e-09, 2.32554356e-07],\n",
       "       [2.27878559e-06, 1.29445521e-08, 1.46292916e-06, 4.37150049e-08,\n",
       "        9.94175911e-01, 2.78768823e-07, 3.64977791e-06, 1.56802253e-05,\n",
       "        1.14691568e-06, 5.79956872e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 對 x_test 中前 5 個樣本進行預測，並返回這些樣本的預測概率。\n",
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53faba4a-77ba-432a-a2fd-2eb0ee80df7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366a8e7-e0a9-4d78-aeb3-bbde023d01e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
