{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math # constant status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[5.5000, 3.0000],\n",
      "        [1.0000, 2.0000]])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[5.5, 3],[1, 2]])\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(3, 4) # 只是分配了内存，但是没有初始化，所以值是隨機的\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what we just did:\n",
    "* We created a tensor using one of the numerous factory methods attached to the torch module.\n",
    "* The tensor itself is 2-dimensional, having 3 rows and 4 columns.\n",
    "* The type of the object returned is `torch. Tensor`, which is an alias for `torch.FloatTensor` ; by default, PyTorch tensors are populated with 32-bit floating point numbers. (More on data types below.)\n",
    "* You will probably see some random-looking values when printing your tensor. The `torch. empty()` call allocates memory for the tensor, but does not initialize it with any values - so what you're seeing is whatever was in memory at the time of allocation.\n",
    "\n",
    "A brief note about tensors and their number of dimensions, and terminology:\n",
    "* You will sometimes see a 1-dimensional tensor called a vector.\n",
    "* Likewise, a 2-dimensional tensor is often referred to as a matrix.\n",
    "* Anything with more than two dimensions is generally just called a tensor.\n",
    "More often than not, you'll want to initialize your tensor with some value. Common cases are all zeros, all ones, or random values, and the torch module provides factory methods for all of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factory methods all do just what you'd expect - we have a tensor full of zeros, another full of ones, and another with random values between 0 and 1.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Random Tensors and Seeding**  \n",
    "Speaking of the random tensor, did you notice the call to `torch-manual seed()` immediately preceding it? Initializing tensors, such as a model's learning weights, with random values is common but there are times - especially in research settings - where you'll want some assurance of the reproducibility of your results. Manually setting your random number generator's seed is the way to do this. Let's look more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[ 1.0757, -1.2086, -0.6922],\n",
      "        [ 2.0419, -1.8508,  2.1626]])\n",
      "tensor([[-1.1257, -0.0057, -1.3975],\n",
      "        [ 1.4364, -0.1068, -0.8413]])\n",
      "tensor([[0.6128, 0.1519, 0.0453],\n",
      "        [0.5035, 0.9978, 0.3884]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.randn(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.randn(2, 3)\n",
    "print(random3)\n",
    "\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should see above is that `random1` and `random3` carry identical values, as do `random2` and `random4`. Manually setting the RNG's seed resets it, so that identical computations depending on random number should, in most settings, provide identical results.  \n",
    "For more information, see the PyTorch documentation on reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tensor Shapes**  \n",
    "Often, when you're performing operations on two or more tensors, they will need to be of the same shape - that is, having the same number of dimensions and the same number of cells in each dimension. For that, we have the `torch.*_like()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 2.5041e-32,  9.3467e-43, -0.0000e+00],\n",
      "         [ 1.6895e+00, -2.0000e+00,  1.6543e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.3972e+00,  2.0000e+00],\n",
      "         [ 1.7108e+00,  0.0000e+00,  1.3881e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 2.5039e-32,  9.3467e-43,  1.0842e-19],\n",
      "         [ 9.6553e-01,  1.0842e-19,  1.9247e+00]],\n",
      "\n",
      "        [[-3.6893e+19,  1.9296e+00,  1.0842e-19],\n",
      "         [ 1.4636e+00,  2.0000e+00,  1.8353e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]],\n",
      "\n",
      "        [[0.5062, 0.8469, 0.2588],\n",
      "         [0.2707, 0.4115, 0.6839]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first new thing in the code cell above is the use of the `.shape` property on a tensor. This property contains a list of the extent of each dimension of a tensor - in our case, `x` is a three-dimensional tensor with shape 2x2x3.  \n",
    "Below that, we call the `.empty_like()`, `.zeros_like()`, `.ones_like()` , and `.rand_ like()` methods. Using the `.shape` property, we can verify that each of these methods returns a tensor of identical dimensionality and extent.  \n",
    "The last way to create a tensor that will cover is to specify its data directly from a PyTorch collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828],[1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(([2, 4, 6], [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.tensor()` is the most straightforward way to create a tensor if you already have data in a Python tuple or list. As shown above, nesting the collections will result in a multi-dimensional tensor.  \n",
    "Note: `torch.tensor()` creates a copy of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Data Types**  \n",
    "Setting the datatype of a tensor is possible a couple of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 1.4051, 10.2103, 18.9010],\n",
      "        [ 4.7172,  3.9587,  6.6549]])\n",
      "tensor([[ 1, 10, 18],\n",
      "        [ 4,  3,  6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float32) * 20\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int64)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set the underlying data type of a tensor is with an optional argument at creation time. In the first line of the cell above, we set `dtype=torch.intl6` for the tensor `a`. When we print `a`, we can see that it's full of `1` rather than `1.` - Python's subtle cue that this is an integer type rather than floating point.  \n",
    "Another thing to notice about printing a is that, unlike when we left `dtype` as the default (32-bit floating point), printing the tensor also specifies its `dtype`.  \n",
    "You may have also spotted that we went from specifying the tensor's shape as a series of integer arguments, to grouping those arguments in a tuple. This is not strictly necessary - PyTorch will take a series of initial, unlabeled integer arguments as a tensor shape - but when adding the optional arguments, it can make your intent more readable.  \n",
    "The other way to set the datatype is with the `.to()` method. In the cell above, we create a random floating point tensor `b` in the usual way. Following that, we create `c` by converting `b` to a 32-bit integer with the `.to()` method. Note that `c` contains all the same values as `b`, but truncated to integers.  \n",
    "Available data types include:  \n",
    "* torch.bool\n",
    "* torch.int8\n",
    "* torch.uint8\n",
    "* torch.int16\n",
    "* torch.int32\n",
    "* torch.int64\n",
    "* torch.half\n",
    "* torch.tloat\n",
    "* torch.double\n",
    "* torch.bfloat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math & Logic with PyTorch Tensors**  \n",
    "Now that you know some of the ways to create a tensor... what can you do with them?\n",
    "\n",
    "Let's look at basic arithmetic first, and how tensors interact with simple scalars:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 2) * 2\n",
    "print(twos)\n",
    "\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "print(threes)\n",
    "\n",
    "fours = twos ** 2\n",
    "print(fours)\n",
    "\n",
    "sqrt2s = twos ** 0.5\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, arithmetic operations between tensors and scalars, such as addition, subtraction, multiplication, division, and exponentiation are distributed over every element of the tensor. Because the output of such an operation will be a tensor, you can chain them together with the usual operator precedence rules, as in the line where we create `threes`.  \n",
    "Similar operations between two tensors also behave like you'd intuitively expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.Tensor(([1, 2], [3, 4]))\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note here that all of the tensors in the previous code cell were of identical shape. What happens when we try to perform a binary operation on tensors if dissimilar shape?  \n",
    "\n",
    "**Note: The following cell throws a run-time error. This is intentional.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1332, 0.0023, 0.4945],\n",
      "        [0.3857, 0.9883, 0.4762]])\n",
      "tensor([[0.7242, 0.0776],\n",
      "        [0.4004, 0.9877],\n",
      "        [0.0352, 0.0905]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(a @ b)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 2)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# print(a @ b)\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.6436e-02, 2.9950e-02],\n",
      "        [9.0789e-04, 9.7623e-01],\n",
      "        [1.7419e-02, 4.3096e-02]])\n"
     ]
    }
   ],
   "source": [
    "print(a.T * b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general case, you cannot operate on tensors of different shape this way, even in a case like the cell above, where the tensors have an identical number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Brief: Tensor Broadcasting**  \n",
    "(Note: if you are familiar with broadcasting semantics(語意) in NumPy ndarrays, you'll find the same rules apply here.)  \n",
    "The exception to the same-shapes rule is tensor broadcasting. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4485, 0.8740, 0.2526, 0.6923],\n",
      "        [0.7545, 0.7746, 0.2330, 0.8441]])\n",
      "tensor([[0.8971, 1.7480, 0.5051, 1.3846],\n",
      "        [1.5091, 1.5491, 0.4660, 1.6881]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the trick here? How is it we got to multiply a 2x4 tensor by a 1x4 tensor?\n",
    "\n",
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example above, the one-row, four-column tensor is multiplied by both rows of the two-row, four-column tensor.\n",
    "\n",
    "This is an important operation in Deep Learning. The common example is multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, and returning a tensor of identical shape - just like our (2, 4) * (1, 4) example above returned a tensor of shape (2, 4).\n",
    "\n",
    "The rules for broadcasting are:\n",
    "* Each tensor must have at least one dimension - no empty tensors.\n",
    "* Comparing the dimension sizes of the two tensors, going from last to first:\n",
    "  * Each dimension must be equal, or\n",
    "  * One of the dimensions must be of size 1, or\n",
    "  * The dimension does not exist in one of the tensors  \n",
    "Tensors of identical shape, of course, are trivially \"broadcastable\", as you saw earlier.\n",
    "\n",
    "Here are some examples of situations that honor the above rules and allow broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]]])\n",
      "tensor([[[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]]])\n",
      "tensor([[[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at the values of each tensor above:  \n",
    "* The multiplication operation that created b was broadcast over every \"layer\" of `a`.\n",
    "* For `c`, the operation was broadcast over ever layer and row of a - every 3-element column is identical.\n",
    "* For `d`, we switched it around - now every row is identical, across layers and columns.\n",
    "\n",
    "For more information on broadcasting, see the PyTorch documentation on the topic.  \n",
    "Here are some examples of attempts at broadcasting that will fail:  \n",
    "\n",
    "**Note: The following cell throws a run-time error. This is intentional.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m     torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# dimensions must match last-to-first\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[0;32m      6\u001b[0m c \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(   \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# both 3rd & 2nd dims different\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(4, 3)    # dimensions must match last-to-first，由後往前比對\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand((0, ))   # can't broadcast with an empty tensor\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Math with Tensors**  \n",
    "PyTorch tensors have over three hundred operations that can be performed on them.\n",
    "\n",
    "Here is a small sample from some of the major categories of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[ 0.0503, -0.6737,  0.4782,  0.8509],\n",
      "        [-0.4908,  0.8738,  0.4975,  0.8212]])\n",
      "tensor([[0.0503, 0.6737, 0.4782, 0.8509],\n",
      "        [0.4908, 0.8738, 0.4975, 0.8212]])\n",
      "tensor([[1., -0., 1., 1.],\n",
      "        [-0., 1., 1., 1.]])\n",
      "tensor([[ 0., -1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.,  0.]])\n",
      "tensor([[ 0.0503, -0.5000,  0.4782,  0.5000],\n",
      "        [-0.4908,  0.5000,  0.4975,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(a)\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5)) # 將值限制在 [-0.5, 0.5] 之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n"
     ]
    }
   ],
   "source": [
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi/4, math.pi/2, math.pi/4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bitwise XOR:\n",
      "tensor([ 1,  5, 11])\n",
      "tensor([ 3,  7, 10])\n",
      "tensor([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([3, 7, 10])\n",
    "print(b)\n",
    "print(c)\n",
    "print(torch.bitwise_xor(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Broadcasted, element-wise equqlity omparison:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 4.]])\n",
      "tensor([[ True, False],\n",
      "        [False,  True]])\n"
     ]
    }
   ],
   "source": [
    "# comparison:\n",
    "print('\\nBroadcasted, element-wise equqlity omparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "# e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "e = torch.tensor([[1., 4.]])  # many comparison ops support broadcasting!\n",
    "\n",
    "print(d)\n",
    "print(e)\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reductions ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# reductions:\n",
    "print('\\nReductions ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation;標準差\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2, 3]))) # fliter unique elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector & Matrices:\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[0.9130, 0.9833],\n",
      "        [0.7706, 0.2174]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.4046, -0.9145],\n",
      "        [-0.9145,  0.4046]]),\n",
      "S=tensor([5.4650, 0.3660]),\n",
      "V=tensor([[-0.5760,  0.8174],\n",
      "        [-0.8174, -0.5760]]))\n"
     ]
    }
   ],
   "source": [
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])        # x unit vectors\n",
    "v2 = torch.tensor([0., 1., 0.])        # y unit vectors\n",
    "m1 = torch.rand(2, 2)                  # random matrix\n",
    "m2 = torch.tensor([[3., 0.],[0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVector & Matrices:')\n",
    "print(torch.cross(v1, v2))             # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.tensor([[1., 2.],[3., 4.]]) # 3 times ml\n",
    "print(m3)\n",
    "print(torch.svd(m3))                   # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small sample of For more details and the full inventory of math functions, have a look at the documentation.\n",
    "\n",
    "**Altering Tensors in Place**  \n",
    "Most binary operations on tensors will return a third, new tensor. When we say `c = a * b` (where `a` and `b` are tensors), the new tensor `c` will occupy a\n",
    "region of memory distinct from the other tensors.\n",
    "\n",
    "There are times, though, that you may wish to alter a tensor in place - for example, if you're doing an element-wise computation where you can discard intermediate values. For this, most of the math functions have a version with an appended underscore (`_`) that will alter a tensor in place.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi/4, math.pi/2, 3 * math.pi/4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))    # this operation creates a new tensor in memory\n",
    "print(a)               # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi/4, math.pi/2, 3 * math.pi/4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))   # note the underscore\n",
    "print(b)               # b has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arithmetic operations, there are functions that behave similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.2038, 0.2881],\n",
      "        [0.2677, 0.3067]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.2038, 1.2881],\n",
      "        [1.2677, 1.3067]])\n",
      "tensor([[1.2038, 1.2881],\n",
      "        [1.2677, 1.3067]])\n",
      "tensor([[0.2038, 0.2881],\n",
      "        [0.2677, 0.3067]])\n",
      "\n",
      "After multiplying:\n",
      "tensor([[0.0415, 0.0830],\n",
      "        [0.0717, 0.0940]])\n",
      "tensor([[0.0415, 0.0830],\n",
      "        [0.0717, 0.0940]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying:')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these in-place arithmetic functions are methods on the `torch.Tensor` object, not attached to the `torch` module like many other functions (e.g.. `torch.sin()`). As you can see from `a.add_(b)`, the calling tensor is the one that gets changed in place.\n",
    "\n",
    "There is another option for placing the result of a computation in an existing, allocated tensor. Many of the methods and functions we've seen so far - including creation methodst - have an `out` argument that lets you specity a tensor to receive the output. If the `out` tensor is the correct shape and `dtype`, this can happen without a new memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "old_id: 2139738519312\n",
      "tensor([[0.5221, 0.4058],\n",
      "        [0.5952, 0.7076]])\n",
      "tensor([[0.9713, 0.3280],\n",
      "        [0.5546, 0.6437]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c) # 獲取 c 的內存 ID\n",
    "\n",
    "print('c:', c)\n",
    "print('old_id:', old_id)\n",
    "d = torch.matmul(a, b, out=c) # 計算矩陣乘法，結果存儲在 c 中\n",
    "print(c)                 # c has changed\n",
    "\n",
    "assert c is d            # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id   # make sure the new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c)  # works for creation too!\n",
    "print(c)                 # c has changed\n",
    "assert id(c) == old_id   # still the same object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copying Tensors**  \n",
    "As with any object in Python, assigning a tensor to a variable makes the variable a label of the tensor, and does not copy it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561 # change a\n",
    "print(b)      # b has changed too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you want a separate copy of the data to work on? The `clone()` method is there for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone() # b is a copy of a\n",
    "\n",
    "assert b is not a     # different objects in memory\n",
    "print(torch.eq(a, b)) # ...but still with the same contents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
